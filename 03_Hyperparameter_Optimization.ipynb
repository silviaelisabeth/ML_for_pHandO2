{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5488a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run function_dbs.py\n",
    "%run MLRegression_dbs.py\n",
    "%matplotlib inline\n",
    "\n",
    "fs, color = 10, 'k'\n",
    "today = ''.join(str(datetime.today()).split(' ')[0].split('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3091cb",
   "metadata": {},
   "source": [
    "## load data set and split into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "aa39b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ML a rule of thumbs says that a suitable size for a data set is >1,000 samples\n",
      "size of data sets:\n",
      "> training data   (2255, 151) \n",
      "> validation data (251, 151)\n"
     ]
    }
   ],
   "source": [
    "# select which ML algorithm for which analyte shall be analysed \n",
    "file_ox = '460nm/20220825_trainingData-O2_balanced_absoluteInt.csv'\n",
    "file_ph = '460nm/20220921_trainingData-pH_balanced_absoluteInt.csv'\n",
    "\n",
    "df_features = pd.read_csv(file_ox, sep='\\t', index_col=0)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# split the data into training and validation data\n",
    "y_truth = df_features.index              # n_samples (3720,) chosen analyte\n",
    "X = df_features                          # shape (n_features, n_samples) with (3720, 151)\n",
    "X_train, X_valid, truth_train, truth_valid = train_test_split(X, y_truth, test_size=0.1, random_state=42)\n",
    "# X_train, X_valid = X_train.T, X_valid.T\n",
    "\n",
    "print('in ML a rule of thumbs says that a suitable size for a data set is >1,000 samples')\n",
    "print('size of data sets:')\n",
    "print('> training data  ', X_train.shape, '\\n> validation data', X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b33bf255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which ML regressor shall be optimized - (1) Random Forest, (2) Decision tree, or (3) XGBoost? 1\n"
     ]
    }
   ],
   "source": [
    "# select which ML algorithm that shall be optimized (HPO)\n",
    "\n",
    "# Random Forest \n",
    "RF = RandomForestRegressor()\n",
    "\n",
    "# model parameter that will be optimized\n",
    "rf_random = {'n_estimators': [1, 25, 50, 75, 100, 150, 200],\n",
    "             'max_features': [1, 2, 3, 4, 5, 7, 10, 20],\n",
    "             'max_depth': [None, 1, 3, 5, 7, 9, 11, 20, 50],\n",
    "             'min_samples_split': [2, 3, 4],\n",
    "             'min_samples_leaf': [1, 2, 4, 6, 8, 10], \n",
    "             'bootstrap': [True, False],  \n",
    "             'max_leaf_nodes': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "             'min_weight_fraction_leaf': [0, 0.1, 0.2, 0.3, 0.4, 0.45]}\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeRegressor() #make_pipeline(preprocessing.StandardScaler(), )\n",
    "\n",
    "# model parameter that will be optimized\n",
    "dt_random = {\"splitter\":[\"best\",\"random\"],\n",
    "             \"max_depth\" : [None, 1,3,5,7,9,11,12],\n",
    "             \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10],\n",
    "             \"min_weight_fraction_leaf\":[0, 0.1,0.2,0.3,0.4,0.45],\n",
    "             \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
    "             \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90]}\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "XG = XGBRegressor()\n",
    "\n",
    "# model parameter that will be optimized\n",
    "xg_random = {'n_estimators': [1, 100, 200, 400],\n",
    "             'max_depth': [1, 5, 7, 9, 13],\n",
    "             'learning_rate': [0, 0.05, 0.1, 0.5],\n",
    "             'min_child_weight': [0, 5, 10, 15, 20]}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "model_id = input('Which ML regressor shall be optimized - (1) Random Forest, (2) Decision tree, or (3) XGBoost? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "01bf3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_id == '1':\n",
    "    model = RF\n",
    "    random_grid = rf_random\n",
    "elif model_id == '2':\n",
    "    model = DT\n",
    "    random_grid = dt_random\n",
    "elif model_id == '3':\n",
    "    model = XG\n",
    "    random_grid = xg_random\n",
    "else:\n",
    "    print('Please specify the model via its number: (1) Random Forest, (2) Decision tree, or (3) XGBoost')\n",
    "    model = None\n",
    "    random_grid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5774d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "# Random search of parameters, using 3 fold cross validation (cv), \n",
    "# search across 100 different combinations, and use all available cores\n",
    "if model != None:\n",
    "    ML_random = RandomizedSearchCV(estimator=model, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    ML_random.fit(X_train, truth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "d1c3d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Best estimation of hyperparameter for RandomForestRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 150,\n",
       " 'min_weight_fraction_leaf': 0,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_leaf_nodes': 70,\n",
       " 'max_features': 10,\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('>> Best estimation of hyperparameter for ' + str(model)[:-2])\n",
    "ML_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c82457",
   "metadata": {},
   "source": [
    "#### Assess model performance with identified HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "cd711d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_id == '1':\n",
    "    model.estimator_params = ML_random.best_params_['n_estimators']\n",
    "    model.min_weight_fraction_leaf = ML_random.best_params_['min_weight_fraction_leaf']\n",
    "    model.min_samples_split = ML_random.best_params_['min_samples_split']\n",
    "    model.min_samples_leaf = ML_random.best_params_['min_samples_leaf']\n",
    "    model.max_leaf_nodes = ML_random.best_params_['max_leaf_nodes']\n",
    "    model.max_features = ML_random.best_params_['max_features']\n",
    "    model.max_depth = ML_random.best_params_['max_depth']\n",
    "    model.bootstrap = ML_random.best_params_['bootstrap']\n",
    "    \n",
    "elif model_id == '2':\n",
    "    model.splitter = ML_random.best_params_['splitter']\n",
    "    model.min_weight_fraction_leaf = ML_random.best_params_['min_weight_fraction_leaf']\n",
    "    model.min_samples_leaf = ML_random.best_params_['min_samples_leaf']\n",
    "    model.max_leaf_nodes = ML_random.best_params_['max_leaf_nodes']\n",
    "    model.max_features = ML_random.best_params_['max_features']\n",
    "    model.max_depth = ML_random.best_params_['max_depth']  \n",
    "    \n",
    "elif model_id == '3':\n",
    "    model.n_estimators = ML_random.best_params_['n_estimators']\n",
    "    model.max_depth = ML_random.best_params_['max_depth']  \n",
    "    model.learning_rate = ML_random.best_params_['learning_rate']\n",
    "    model.min_child_weight = ML_random.best_params_['min_child_weight']\n",
    "            \n",
    "regressor = make_pipeline(preprocessing.StandardScaler(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "2f1ddfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model performance with optimized hyper-parameters\n",
    "[model_, y_predT, y_pred, para] = _fitPredict(model=regressor, X_train=X_train, truth_train=truth_train, \n",
    "                                              X_valid=X_valid, truth_valid=truth_valid)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "para = pd.DataFrame(para, index=['mae', 'rmse', 'sdz'])\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa084fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
